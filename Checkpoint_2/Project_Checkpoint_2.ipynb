{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULsf1jkyi3sn"
   },
   "source": [
    "# *Init*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "NxRQB7x6VRih"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a1A9ep3tgSF",
    "outputId": "3b8a02a1-7f07-486f-e3f1-e6fe0164729f"
   },
   "outputs": [],
   "source": [
    "# #@title Mount Data\n",
    "# # Mount data drive\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/', force_remount=True)\n",
    "# %cd /content/drive/MyDrive/CPSC4300-ADS-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TVFToS1fJLn"
   },
   "source": [
    "# **Unhealthy Tree Detection in Segmented Drone Footage via Machine Learning**\n",
    "**Clemson University | Fall 2023**<br>\n",
    "**Authors:** Scott Logan, Lisa Umatoni, Mostafa Saberian, Ian McCall, Neil Kuehn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CPi2tAImOSu"
   },
   "source": [
    "NOTES:\n",
    "\n",
    "Look into adapting a pretrained model, such as VGG or Resnet, and retraining only the last few layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gYgXQL_jooH"
   },
   "source": [
    "# **Project Goals!**\n",
    "\n",
    "# To Do\n",
    "- Finish Data Cleaning Methods\n",
    "- Implement CNNs 2,3\n",
    "- Try implementing 4\n",
    "- Look into adapting a pretrained model, such as VGG or Resnet, and retraining only the last few layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Onhj_dQVjYmi"
   },
   "source": [
    "# **Data Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4o8pbcejfUE"
   },
   "source": [
    "The provided data set is a total of 83 images of trees, with 45 healthy trees and 28 sick tree images given. The main unit of analysis for determining whether a tree is sick or not is color. Healthy trees are greener and darker, whereas sick trees are yellowed and lighter. \n",
    "\n",
    "Very light areas, such as bare tree branches, are not counted as sick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3S3N6DOimCZ"
   },
   "source": [
    "# **Data Cleaning Strategies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnh8cb4ejJ5S"
   },
   "source": [
    "In a photograph, there is plenty of noise that may distract a Machine Learning Model from the data that is being represented. As such, it may be beneficial to perform data cleaning steps before training or predicting using the model.\n",
    "\n",
    "\n",
    "We have decided to implement the following data cleaning steps, and test the model's performance using various combinations of these steps:\n",
    "\n",
    "**Greyscale:** Normalize each pixel into a single grey value\n",
    "\n",
    "**Isolate Hue:** Isolate the Red channel of each pixel\n",
    "> Tree health is largely defined by yellowing, which in an RGB value is defined by an increase in the Red value. As such, we may be interested primarily in the Red channel, and may increase model accuracy by isolating or at least exaggerating the Red channel of images during processing.\n",
    "\n",
    "**Omit Values Beyond Range of Interest:** Remove information likely to confuse the model\n",
    "> Areas of images with red values that are too high are likely to be unrelated to tree data, so they should be omitted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "7_JyuzU2W1-a"
   },
   "outputs": [],
   "source": [
    "#@title Greyscale\n",
    "def greyscale(input_img):\n",
    "    \n",
    "    output_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
    "    return output_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "9hRN35l-gQv9"
   },
   "outputs": [],
   "source": [
    "#@title Normalize Saturation and Value:\n",
    "def normalize_saturation_value(input_img):\n",
    "    # Convert the image from BGR to HSV color space\n",
    "    hsv_image = image.rgb_to_hsv(input_img)\n",
    "    #hsv_image = cv2.cvtColor(input_img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Split the HSV image into separate channels\n",
    "    h, s, v = cv2.split(hsv_image)\n",
    "\n",
    "    # Normalize the saturation and value channels\n",
    "    s = np.uint8(np.clip((s * 1.2), 0, 255))\n",
    "    v = np.uint8(np.clip((v * 1.2), 0, 255))\n",
    "\n",
    "    # Merge the normalized channels back into an HSV image\n",
    "    hsv_image = cv2.merge([h, s, v])\n",
    "\n",
    "    # Convert the HSV image back to BGR color space\n",
    "    output_image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GwsxFs539HWH"
   },
   "outputs": [],
   "source": [
    "#@title Isolate Hue:\n",
    "def isolate_hue(input_img):\n",
    "    \n",
    "    # Exaggerate the red channel by decreasing blue and green by 80%\n",
    "    output_img = input_img[:, :, :2] * 0.2\n",
    "\n",
    "    return output_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EBhu2OF_hvTE"
   },
   "outputs": [],
   "source": [
    "#@title Omit Unwanted Pixel Data Beyond Range of Interest\n",
    "\n",
    "def omit_unwanted_ranges(input_img):\n",
    "\n",
    "    output_img = input_img\n",
    "    return output_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "IIYAnMEhjHn3"
   },
   "outputs": [],
   "source": [
    "#@title Normalize Pixels\n",
    "\n",
    "def normalize_pixels(input_img):\n",
    "    img_arr = image.img_to_array(input_img)\n",
    "    img_arr / 255.0\n",
    "\n",
    "    output_img = image.array_to_img(img_arr)\n",
    "    return output_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "dYYmUAo9jROA"
   },
   "outputs": [],
   "source": [
    "#@title Resize Images\n",
    "\n",
    "def resize_img(input_img, targetwidth, targetheight):\n",
    "    output_img = tf.image.resize(input_img, [targetwidth, targetheight])\n",
    "    return output_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "-9bQNiNYeUUr"
   },
   "outputs": [],
   "source": [
    "#@title Cleaning Function\n",
    "\n",
    "def clean_data(input_img, GS=False, NSV=False, IH=False, OU=False, NP=True, RI=True, resize_W=2000, resize_H=1125):\n",
    "    clean_img = input_img\n",
    "    if GS:\n",
    "        clean_img = greyscale(clean_img)\n",
    "    if NSV:\n",
    "        clean_img = normalize_saturation_value(clean_img)\n",
    "    if IH:\n",
    "        clean_img = isolate_hue(clean_img)\n",
    "    if OU:\n",
    "        clean_img = omit_unwanted_ranges(clean_img)\n",
    "    if NP:\n",
    "        clean_img = normalize_pixels(clean_img)\n",
    "    if RI:\n",
    "        clean_img = resize_img(clean_img, 2000, 1125)\n",
    "\n",
    "    output_img = clean_img\n",
    "    return output_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drkqG8hPivTd"
   },
   "source": [
    "# **Selected Model:** Convolutional Neural Network\n",
    "\n",
    "We considered a number of different models for this project, choosing CNN as our initial model selection:\n",
    "- **-> Convolutional Neural Network (CNN):** Suitable for classifying photos by visible features, which we plan to use by training the CNN to detect color patterns typical of sick trees.\n",
    "- **Classification Model:** Suitable for binary response values, which may be useful to classify healthy vs. sick.\n",
    "- **Clustering Model:** May be useful to detect multiple instances of sick trees within an image, using elbow method to determine number of sick tree instances.\n",
    "\n",
    "\n",
    "# Model Architecture\n",
    "\n",
    "A few different CNN implementations will be tested, primarily as an exploration of how CNNs work.\n",
    "\n",
    "- Implementation 1: Basic Brute-Force Approach\n",
    " - Training Data:\n",
    "   - Cleaning Steps: Resize Images, Normalize Pixels\n",
    " - Two Convolutional Layers: \n",
    " - Two Pooling Layers: \n",
    " - Two Dense Layers:\n",
    "   - 128-unit Relu layer: Learn non-linear transformations of features to capture complex relationships between features.\n",
    "   - 1-unit Sigmoid layer: Filters output into a single binary value: 0 for healthy (lacks sick features), 1 for sick (contains sick features)\n",
    " - \n",
    "\n",
    "- Model 2: Cleaned Approaches\n",
    "\n",
    "\n",
    "- Model 3: Noisy Approach\n",
    "\n",
    "\n",
    "- Model 4: Repurpose trailed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fD1l_um-leEw"
   },
   "source": [
    "# **Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fv7jR6-iyMSs"
   },
   "source": [
    "In order to train and test our model, we need to construct a feature matrix on which to train our data. For this CNN model, the input data needs to come as an array of tuples defined as (id, feature). Images of sick regions of trees will be paired with an ID of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Data Cleaning step for Model\n",
    "default_width, default_height = 400, 225\n",
    "def clean_data(input_img, resize_width=default_width, resize_height=default_height):\n",
    "    clean_img = input_img\n",
    "    \n",
    "    # Apply Data Cleaning Functions\n",
    "    clean_img = normalize_pixels(clean_img)\n",
    "    clean_img = resize_img(clean_img, resize_width, resize_height)\n",
    "    \n",
    "    output_img = clean_img\n",
    "    return output_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "A8nfEDJtyLs2",
    "outputId": "51f5c8d7-4183-4f31-a803-85cc96973391"
   },
   "outputs": [],
   "source": [
    "# Read Images\n",
    "\n",
    "tree_imgs = []\n",
    "healthy_tree_imgs = []\n",
    "sick_tree_imgs = []\n",
    "sick_tree_features = []\n",
    "\n",
    "# Read files and perform data cleaning steps\n",
    "dir_name = \"data/healthy\"\n",
    "for file in os.listdir(dir_name):\n",
    "    img_path = os.path.join(dir_name, file)\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # img = cv2.imread(os.path.join(dir_name, file))\n",
    "    clean_img = clean_data(img)\n",
    "    tree_imgs.append(clean_img)\n",
    "    healthy_tree_imgs.append(clean_img)\n",
    "\n",
    "dir_name = \"data/sick\"\n",
    "for file in os.listdir(dir_name):\n",
    "    img_path = os.path.join(dir_name, file)\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    clean_img = clean_data(img)\n",
    "    tree_imgs.append(clean_img)\n",
    "    sick_tree_imgs.append(clean_img)\n",
    "\n",
    "# dir_name = \"data/sick_features\"    \n",
    "# for file in os.listdir(dir_name):\n",
    "#     img = cv2.imread(os.path.join(dir_name, file))\n",
    "#     clean_img = clean_data(img, 100, 100)\n",
    "#     sick_tree_features.append(clean_img)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "Lvd9ggp8W83_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theca\\AppData\\Local\\Temp\\ipykernel_5436\\2672586335.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  feature_mx = np.array([(0, img) for img in healthy_tree_imgs] + [(1, img) for img in sick_tree_imgs])\n"
     ]
    }
   ],
   "source": [
    "# Create feature matrix and split into training and testing data\n",
    "feature_mx = np.array([(0, img) for img in healthy_tree_imgs] + [(1, img) for img in sick_tree_imgs])\n",
    "\n",
    "train_data, test_data = train_test_split(feature_mx)\n",
    "X_train = train_data[0]\n",
    "y_train = test_data[1]\n",
    "\n",
    "X_test = test_data[0]\n",
    "y_test = test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "G83U4E10PXN8"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     21\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[1;32m~\\.conda\\envs\\CPSC-4300-Proj\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.conda\\envs\\CPSC-4300-Proj\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(default_width, default_height, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[[[29, 73, 58], [39, 84, 68], [34, 81, 65], [4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[[[24, 41, 32], [21, 37, 30], [21, 37, 30], [2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[[[51, 46, 47], [36, 31, 32], [40, 38, 38], [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[[[53, 73, 74], [29, 49, 50], [89, 110, 112], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[[[110, 153, 158], [91, 133, 133], [93, 135, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>[[[59, 68, 78], [160, 166, 177], [149, 155, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>[[[38, 52, 58], [39, 53, 59], [79, 94, 97], [7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>[[[32, 53, 50], [35, 56, 53], [36, 59, 55], [4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>[[[58, 89, 86], [62, 91, 88], [63, 90, 87], [4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>[[[25, 34, 31], [15, 24, 21], [17, 26, 23], [2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>[[[148, 186, 191], [108, 147, 151], [115, 159,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>[[[35, 69, 58], [28, 60, 49], [31, 59, 50], [1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>[[[71, 84, 62], [73, 86, 64], [65, 78, 57], [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>[[[63, 80, 77], [70, 87, 84], [75, 92, 89], [8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>[[[23, 27, 21], [19, 24, 18], [20, 27, 20], [2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                                img\n",
       "0      1  [[[29, 73, 58], [39, 84, 68], [34, 81, 65], [4...\n",
       "1      0  [[[24, 41, 32], [21, 37, 30], [21, 37, 30], [2...\n",
       "2      1  [[[51, 46, 47], [36, 31, 32], [40, 38, 38], [5...\n",
       "3      1  [[[53, 73, 74], [29, 49, 50], [89, 110, 112], ...\n",
       "4      1  [[[110, 153, 158], [91, 133, 133], [93, 135, 1...\n",
       "5      0  [[[59, 68, 78], [160, 166, 177], [149, 155, 16...\n",
       "6      0  [[[38, 52, 58], [39, 53, 59], [79, 94, 97], [7...\n",
       "7      0  [[[32, 53, 50], [35, 56, 53], [36, 59, 55], [4...\n",
       "8      0  [[[58, 89, 86], [62, 91, 88], [63, 90, 87], [4...\n",
       "9      0  [[[25, 34, 31], [15, 24, 21], [17, 26, 23], [2...\n",
       "10     0  [[[148, 186, 191], [108, 147, 151], [115, 159,...\n",
       "11     0  [[[35, 69, 58], [28, 60, 49], [31, 59, 50], [1...\n",
       "12     1  [[[71, 84, 62], [73, 86, 64], [65, 78, 57], [5...\n",
       "13     0  [[[63, 80, 77], [70, 87, 84], [75, 92, 89], [8...\n",
       "14     1  [[[23, 27, 21], [19, 24, 18], [20, 27, 20], [2..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = pd.DataFrame(test_data, columns=['label', 'img'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>0</td>\n",
       "      <td>[[[29, 73, 58], [39, 84, 68], [34, 81, 65], [4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                                img\n",
       "count      15                                                 15\n",
       "unique      2                                                 15\n",
       "top         0  [[[29, 73, 58], [39, 84, 68], [34, 81, 65], [4...\n",
       "freq        9                                                  1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qL4WiqIllya"
   },
   "source": [
    "# **Results and Discussion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0TVFToS1fJLn",
    "8gYgXQL_jooH"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
