{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULsf1jkyi3sn"
   },
   "source": [
    "# *Init*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NxRQB7x6VRih"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a1A9ep3tgSF",
    "outputId": "3b8a02a1-7f07-486f-e3f1-e6fe0164729f"
   },
   "outputs": [],
   "source": [
    "# #@title Mount Data\n",
    "# # Mount data drive\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/', force_remount=True)\n",
    "# %cd /content/drive/MyDrive/CPSC4300-ADS-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TVFToS1fJLn"
   },
   "source": [
    "# **Unhealthy Tree Detection in Segmented Drone Footage via Machine Learning**\n",
    "**Clemson University | Fall 2023**<br>\n",
    "**Authors:** Scott Logan, Lisa Umatoni, Mostafa Saberian, Ian McCall, Neil Kuehn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CPi2tAImOSu"
   },
   "source": [
    "NOTES:\n",
    "\n",
    "Look into adapting a pretrained model, such as VGG or Resnet, and retraining only the last few layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gYgXQL_jooH"
   },
   "source": [
    "# **Project Goals!**\n",
    "\n",
    "# To Do\n",
    "- Finish Data Cleaning Methods\n",
    "- Implement CNNs 2,3\n",
    "- Try implementing 4\n",
    "- Look into adapting a pretrained model, such as VGG or Resnet, and retraining only the last few layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Onhj_dQVjYmi"
   },
   "source": [
    "# **Data Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4o8pbcejfUE"
   },
   "source": [
    "The provided data set is a total of 83 images of trees, with 45 healthy trees and 28 sick tree images given. The main unit of analysis for determining whether a tree is sick or not is color. Healthy trees are greener and darker, whereas sick trees are yellowed and lighter. \n",
    "\n",
    "Very light areas, such as bare tree branches, are not counted as sick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3S3N6DOimCZ"
   },
   "source": [
    "# **Data Cleaning Strategies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnh8cb4ejJ5S"
   },
   "source": [
    "In a photograph, there is plenty of noise that may distract a Machine Learning Model from the data that is being represented. As such, it may be beneficial to perform data cleaning steps before training or predicting using the model.\n",
    "\n",
    "\n",
    "We have decided to implement the following data cleaning steps, and test the model's performance using various combinations of these steps:\n",
    "\n",
    "**Greyscale:** Normalize each pixel into a single grey value\n",
    "\n",
    "**Isolate Hue:** Isolate the Red channel of each pixel\n",
    "> Tree health is largely defined by yellowing, which in an RGB value is defined by an increase in the Red value. As such, we may be interested primarily in the Red channel, and may increase model accuracy by isolating or at least exaggerating the Red channel of images during processing.\n",
    "\n",
    "**Omit Values Beyond Range of Interest:** Remove information likely to confuse the model\n",
    "> Areas of images with red values that are too high are likely to be unrelated to tree data, so they should be omitted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7_JyuzU2W1-a"
   },
   "outputs": [],
   "source": [
    "#@title Greyscale\n",
    "def greyscale(input_img):\n",
    "    \n",
    "    output_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
    "    return output_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9hRN35l-gQv9"
   },
   "outputs": [],
   "source": [
    "#@title Normalize Saturation and Value:\n",
    "def normalize_saturation_value(input_img):\n",
    "    # Convert the image from BGR to HSV color space\n",
    "    hsv_image = cv2.cvtColor(input_img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Split the HSV image into separate channels\n",
    "    h, s, v = cv2.split(hsv_image)\n",
    "\n",
    "    # Normalize the saturation and value channels\n",
    "    s = np.uint8(np.clip((s * 1.2), 0, 255))\n",
    "    v = np.uint8(np.clip((v * 1.2), 0, 255))\n",
    "\n",
    "    # Merge the normalized channels back into an HSV image\n",
    "    hsv_image = cv2.merge([h, s, v])\n",
    "\n",
    "    # Convert the HSV image back to BGR color space\n",
    "    output_image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GwsxFs539HWH"
   },
   "outputs": [],
   "source": [
    "#@title Isolate Hue:\n",
    "def isolate_hue(input_img):\n",
    "    \n",
    "    # Exaggerate the red channel by decreasing blue and green by 80%\n",
    "    output_img = input_img[:, :, :2] * 0.2\n",
    "\n",
    "    return output_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EBhu2OF_hvTE"
   },
   "outputs": [],
   "source": [
    "#@title Omit Unwanted Pixel Data Beyond Range of Interest\n",
    "\n",
    "def omit_unwanted_ranges(input_img):\n",
    "\n",
    "    output_img = input_img\n",
    "    return output_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IIYAnMEhjHn3"
   },
   "outputs": [],
   "source": [
    "#@title Normalize Pixels\n",
    "\n",
    "def normalize_pixels(input_img):\n",
    "\n",
    "    input_img / 255.0\n",
    "\n",
    "    output_img = input_img\n",
    "    return output_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "dYYmUAo9jROA"
   },
   "outputs": [],
   "source": [
    "#@title Resize Images\n",
    "\n",
    "def resize_img(input_img, targetwidth, targetheight):\n",
    "    output_img = cv2.resize(input_img, (targetwidth, targetheight))\n",
    "    return output_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "-9bQNiNYeUUr"
   },
   "outputs": [],
   "source": [
    "#@title Cleaning Function\n",
    "\n",
    "def clean_data(input_img, GS=False, NSV=False, IH=False, OU=False, NP=True, RI=True, resize_W=2000, resize_H=1125):\n",
    "    clean_img = input_img\n",
    "    if GS:\n",
    "        clean_img = greyscale(clean_img)\n",
    "    if NSV:\n",
    "        clean_img = normalize_saturation_value(clean_img)\n",
    "    if IH:\n",
    "        clean_img = isolate_hue(clean_img)\n",
    "    if OU:\n",
    "        clean_img = omit_unwanted_ranges(clean_img)\n",
    "    if NP:\n",
    "        clean_img = normalize_pixels(clean_img)\n",
    "    if RI:\n",
    "        clean_img = resize_img(clean_img, 2000, 1125)\n",
    "\n",
    "    output_img = clean_img\n",
    "    return output_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drkqG8hPivTd"
   },
   "source": [
    "# **Selected Model:** Convolutional Neural Network\n",
    "\n",
    "We considered a number of different models for this project, choosing CNN as our initial model selection:\n",
    "- **-> Convolutional Neural Network (CNN):** Suitable for classifying photos by visible features, which we plan to use by training the CNN to detect color patterns typical of sick trees.\n",
    "- **Classification Model:** Suitable for binary response values, which may be useful to classify healthy vs. sick.\n",
    "- **Clustering Model:** May be useful to detect multiple instances of sick trees within an image, using elbow method to determine number of sick tree instances.\n",
    "\n",
    "\n",
    "# Model Architecture\n",
    "\n",
    "A few different CNN implementations will be tested, primarily as an exploration of how CNNs work.\n",
    "\n",
    "- Implementation 1: Basic Brute-Force Approach\n",
    " - Training Data:\n",
    "   - Cleaning Steps: Resize Images, Normalize Pixels\n",
    " - Two Convolutional Layers: \n",
    " - Two Pooling Layers: \n",
    " - Two Dense Layers:\n",
    "   - 128-unit Relu layer: Learn non-linear transformations of features to capture complex relationships between features.\n",
    "   - 1-unit Sigmoid layer: Filters output into a single binary value: 0 for healthy (lacks sick features), 1 for sick (contains sick features)\n",
    " - \n",
    "\n",
    "- Model 2: Cleaned Approaches\n",
    "\n",
    "\n",
    "- Model 3: Noisy Approach\n",
    "\n",
    "\n",
    "- Model 4: Repurpose trailed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fD1l_um-leEw"
   },
   "source": [
    "# **Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fv7jR6-iyMSs"
   },
   "source": [
    "In order to train and test our model, we need to construct a feature matrix on which to train our data. For this CNN model, the input data needs to come as an array of tuples defined as (id, feature). Images of sick regions of trees will be paired with an ID of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "A8nfEDJtyLs2",
    "outputId": "51f5c8d7-4183-4f31-a803-85cc96973391"
   },
   "outputs": [],
   "source": [
    "# Read Images\n",
    "\n",
    "tree_imgs = []\n",
    "healthy_tree_imgs = []\n",
    "sick_tree_imgs = []\n",
    "sick_tree_features = []\n",
    "\n",
    "# Read files and perform data cleaning steps\n",
    "dir_name = \"data/healthy\"\n",
    "for file in os.listdir(dir_name):\n",
    "    img = cv2.imread(os.path.join(dir_name, file))\n",
    "    clean_img = clean_data(img)\n",
    "    tree_imgs.append(clean_img)\n",
    "    healthy_tree_imgs.append(clean_img)\n",
    "\n",
    "dir_name = \"data/sick\"\n",
    "for file in os.listdir(dir_name):\n",
    "    img = cv2.imread(os.path.join(dir_name, file))\n",
    "    clean_img = clean_data(img)\n",
    "    tree_imgs.append(clean_img)\n",
    "    sick_tree_imgs.append(clean_img)\n",
    "\n",
    "# dir_name = \"data/sick_features\"    \n",
    "# for file in os.listdir(dir_name):\n",
    "#     img = cv2.imread(os.path.join(dir_name, file))\n",
    "#     clean_img = clean_data(img, 100, 100)\n",
    "#     sick_tree_features.append(clean_img)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Lvd9ggp8W83_"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create feature matrix and split into training and testing data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m feature_mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([(\u001b[38;5;241m1\u001b[39m, sick_feat) \u001b[38;5;28;01mfor\u001b[39;00m sick_feat \u001b[38;5;129;01min\u001b[39;00m sick_tree_features])\n\u001b[1;32m----> 4\u001b[0m (X_train, y_train), (X_test, y_test) \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\CPSC-4300-Proj\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2562\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2559\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2561\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2562\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2564\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\CPSC-4300-Proj\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2236\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2233\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2240\u001b[0m     )\n\u001b[0;32m   2242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# Create feature matrix and split into training and testing data\n",
    "feature_mx = np.array([(1, sick_feat) for sick_feat in sick_tree_features])\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = train_test_split(feature_mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "G83U4E10PXN8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val))\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     24\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Train model on input data\n",
    "\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(1125, 2000, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qL4WiqIllya"
   },
   "source": [
    "# **Results and Discussion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0TVFToS1fJLn",
    "8gYgXQL_jooH"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
